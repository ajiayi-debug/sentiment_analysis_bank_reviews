{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from keybert import KeyBERT\n",
    "from rake_nltk import Rake\n",
    "import nltk\n",
    "from transformers import pipeline\n",
    "import seaborn as sns\n",
    "import mysql.connector\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load database configuration from JSON file\n",
    "with open('config.json') as config_file:\n",
    "    config = json.load(config_file)['database']\n",
    "\n",
    "# Establish a connection using the loaded configuration\n",
    "cnx = mysql.connector.connect(**config)\n",
    "\n",
    "# Define your query\n",
    "query = f\"SELECT * FROM combined_reviews\"\n",
    "\n",
    "# Use pandas to load sql query into a DataFrame\n",
    "dataset = pd.read_sql(query, con=cnx)\n",
    "\n",
    "# Don't forget to close the connection when done\n",
    "cnx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset.score.isna()==True]\n",
    "\n",
    "sns.histplot(dataset.score) # uneven spread of ratings (mostly 1 or 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided that above 3 stars == positive (1), rest is negative (-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_sentiment = []\n",
    "for score in dataset.score:\n",
    "  if score > 3:\n",
    "    score_sentiment.append(1) # Positive\n",
    "  # elif score == 3:\n",
    "  #   score_sentiment.append(0) # Neutral\n",
    "  else:\n",
    "    score_sentiment.append(-1) # Negative\n",
    "\n",
    "dataset['score_sentiment'] = score_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created a predetermined_sentiment table for a benchline to gauge accuracy of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load database configuration from JSON file\n",
    "with open('config.json') as config_file:\n",
    "    config = json.load(config_file)['database']\n",
    "\n",
    "# Construct the connection string using the loaded configuration\n",
    "user = config['user']\n",
    "password = config['password']\n",
    "host = config['host']\n",
    "dbname = config['database']\n",
    "port = config['port']\n",
    "conn_string = f'mysql+mysqlconnector://{user}:{password}@{host}:{port}/{dbname}'\n",
    "\n",
    "# Create an engine\n",
    "engine = create_engine(conn_string)\n",
    "\n",
    "dataset.to_sql(name='our_predetermined_sentiment', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_accuracy(model):\n",
    "  sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model)\n",
    "  count = 0\n",
    "\n",
    "  for row in range(len(dataset.content)):\n",
    "    pred_sentiment = sentiment_pipeline(dataset.content[row])[0]['label']\n",
    "    pred_sentiment = define(pred_sentiment)\n",
    "    if pred_sentiment == dataset.score_sentiment[row]:\n",
    "      count += 1\n",
    "\n",
    "  acc = count/len(dataset.content)\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# baseline model\n",
    "baseline_model = 'distilbert/distilbert-base-uncased-finetuned-sst-2-english'\n",
    "\n",
    "def define(pred):\n",
    "  if pred == 'POSITIVE': return 1\n",
    "  # elif pred == 'NEGATIVE': return -1\n",
    "  else: return -1\n",
    "\n",
    "sentiment_accuracy(baseline_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fin model\n",
    "fin_model = 'yiyanghkust/finbert-tone'\n",
    "\n",
    "def define(pred):\n",
    "  if pred == 'Positive': return 1\n",
    "  # elif pred == 'Negative': return -1\n",
    "  else: return -1\n",
    "\n",
    "sentiment_accuracy(fin_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FInetuning of bert model (better performing model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json') as config_file:\n",
    "    config = json.load(config_file)['database']\n",
    "\n",
    "# Establish a connection using the loaded configuration\n",
    "cnx = mysql.connector.connect(**config)\n",
    "\n",
    "# Define your query\n",
    "query = f\"SELECT * FROM datasettest\"\n",
    "\n",
    "# Use pandas to load sql query into a DataFrame\n",
    "datasettest = pd.read_sql(query, con=cnx)\n",
    "\n",
    "\n",
    "# Don't forget to close the connection when done\n",
    "cnx.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load database configuration from JSON file\n",
    "with open('config.json') as config_file:\n",
    "    config = json.load(config_file)['database']\n",
    "\n",
    "# Establish a connection using the loaded configuration\n",
    "cnx = mysql.connector.connect(**config)\n",
    "\n",
    "# Define your query\n",
    "query = f\"SELECT * FROM datasettrain\"\n",
    "\n",
    "# Use pandas to load sql query into a DataFrame\n",
    "datasettrain = pd.read_sql(query, con=cnx)\n",
    "\n",
    "# Don't forget to close the connection when done\n",
    "cnx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_sentiment_train = []\n",
    "for score in datasettrain.score:\n",
    "  if score > 3:\n",
    "    score_sentiment_train.append(1) # Positive\n",
    "  else:\n",
    "    score_sentiment_train.append(0) # Negative\n",
    "\n",
    "score_sentiment_test = []\n",
    "for score in datasettest.score:\n",
    "  if score > 3:\n",
    "    score_sentiment_test.append(1) # Positive\n",
    "  else:\n",
    "    score_sentiment_test.append(0) # Negative\n",
    "\n",
    "datasettrain['score_sentiment'] = score_sentiment_train\n",
    "datasettest['score_sentiment']=score_sentiment_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # baseline model train\n",
    "# baseline_model_train = 'distilbert/distilbert-base-uncased-finetuned-sst-2-english'\n",
    "\n",
    "# finetune=datasettrain.drop(columns=['replyContent','thumbsUpCount','score'])\n",
    "# finetune=finetune.rename(columns={\n",
    "#     'content':'text',\n",
    "#     'score_sentiment':'label'\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load database configuration from JSON file\n",
    "# with open('config.json') as config_file:\n",
    "#     config = json.load(config_file)['database']\n",
    "\n",
    "# # Construct the connection string using the loaded configuration\n",
    "# user = config['user']\n",
    "# password = config['password']\n",
    "# host = config['host']\n",
    "# dbname = config['database']\n",
    "# port = config['port']\n",
    "# conn_string = f'mysql+mysqlconnector://{user}:{password}@{host}:{port}/{dbname}'\n",
    "\n",
    "# # Create an engine\n",
    "# engine = create_engine(conn_string)\n",
    "\n",
    "# finetune.to_sql(name='finetune', con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Load your finetuning file into a Pandas DataFrame\n",
    "# df = finetune\n",
    "# # Columns to remove\n",
    "# columns_to_remove = ['Unnamed: 0.1', 'Unnamed: 0', '__index_level_0__']\n",
    "\n",
    "# # Only drop columns which are present in the DataFrame\n",
    "# df = df.drop(columns=[col for col in columns_to_remove if col in df.columns])\n",
    "# # Step 2: Convert the DataFrame to a Hugging Face `Dataset`\n",
    "# dataset_huggingface = Dataset.from_pandas(df)\n",
    "\n",
    "# # Assuming 'df' is your loaded DataFrame and 'dataset' is the Dataset you created\n",
    "\n",
    "# # Split your DataFrame into training and testing sets first\n",
    "# train_df, test_df = train_test_split(df, test_size=0.1)  # For example, 10% as test\n",
    "\n",
    "# # Convert the train and test DataFrames to Datasets\n",
    "# train_dataset = Dataset.from_pandas(train_df)\n",
    "# train_dataset = train_dataset.remove_columns('__index_level_0__')\n",
    "# test_dataset = Dataset.from_pandas(test_df)\n",
    "# test_dataset = test_dataset.remove_columns('__index_level_0__')\n",
    "\n",
    "# # Convert to DatasetDict (if needed for convenience)\n",
    "# dataset_dict = DatasetDict({\n",
    "#     'train': train_dataset,\n",
    "#     'test': test_dataset\n",
    "# })\n",
    "# # Shuffle and select small subsets for fine-tuning (as examples)\n",
    "# small_train_dataset = dataset_dict['train'].shuffle(seed=42).select(range(1008))\n",
    "# small_test_dataset = dataset_dict['test'].shuffle(seed=42).select(range(112))\n",
    "\n",
    "# # Example output\n",
    "# print(small_train_dataset[0])\n",
    "# print(small_test_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(baseline_model_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_function(examples):\n",
    "#     return tokenizer(examples['text'], truncation=True)\n",
    "\n",
    "# tokenized_train = small_train_dataset.map(preprocess_function, batched=True)\n",
    "# tokenized_test = small_test_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DataCollatorWithPadding\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForSequenceClassification\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(baseline_model_train, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the evaluation metrics\n",
    "# import numpy as np\n",
    "# from datasets import load_metric\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     load_accuracy = load_metric(\"accuracy\")\n",
    "#     load_f1 = load_metric(\"f1\")\n",
    "\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "#     f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "#     return {\"accuracy\": accuracy, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Log in to your Hugging Face account\n",
    "# # Get your API token here https://huggingface.co/settings/token\n",
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# repo_name = \"finetuning-sentiment-model-bank_reviews-otherbank\"\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=repo_name,\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     num_train_epochs=2,\n",
    "#     weight_decay=0.01,\n",
    "#     save_strategy=\"epoch\",\n",
    "#     push_to_hub=True,\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_train,\n",
    "#     eval_dataset=tokenized_test,\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Upload the model to the Hub\n",
    "# trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997953772544861},\n",
       " {'label': 'NEGATIVE', 'score': 0.9922934770584106}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run inferences with your new model using Pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "sentiment_model = pipeline(model=\"ajiayi/finetuning-sentiment-model-bank_reviews-otherbank\")\n",
    "\n",
    "sentiment_model([\"I love this move\", \"This movie sucks!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_accuracy_test(model):\n",
    "  sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model)\n",
    "  count = 0\n",
    "\n",
    "  for row in range(len(datasettest.content)):\n",
    "    pred_sentiment = sentiment_pipeline(datasettest.content[row])[0]['label']\n",
    "    pred_sentiment = define(pred_sentiment)\n",
    "    if pred_sentiment == datasettest.score_sentiment[row]:\n",
    "      count += 1\n",
    "\n",
    "  acc = count/len(datasettest.content)\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9791044776119403"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finetuned version of baseline model\n",
    "\n",
    "# baseline model train\n",
    "baseline_model_finetune = 'ajiayi/finetuning-sentiment-model-bank_reviews-otherbank'\n",
    "\n",
    "def define(pred):\n",
    "  if pred == 'POSITIVE': return 1\n",
    "  elif pred == 'NEGATIVE': return 0\n",
    "\n",
    "\n",
    "sentiment_accuracy_test(baseline_model_finetune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text analysis to obtain intent/insight\n",
    "Possible ideas:\n",
    "- Topic modelling\n",
    "- Text summarisation\n",
    "\n",
    "Keyword extraction techniques: https://www.analyticsvidhya.com/blog/2022/03/keyword-extraction-methods-from-documents-in-nlp/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text summarization, doesn't really work since sentences are too short\n",
    "summarizer = pipeline(\"summarization\", min_length=0, max_length=15)\n",
    "summarizer(dataset.content[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic modelling, idk.. doesn't seem to work too well on predefined topic list\n",
    "topic_model = BERTopic.load(\"davanstrien/transformers_issues_topics\")\n",
    "topic, prob = topic_model.transform(dataset.content[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword extraction, keyBERT\n",
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(dataset.content[1])\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword extraction, rake-nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "r=Rake()\n",
    "r.extract_keywords_from_text(dataset.content[1])\n",
    "r.get_ranked_phrases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modelling (for data analysis) https://huggingface.co/heyitskim1912/TopicModelling\n",
    "If we use gpt to reply to reviews, technically dunnid topic modelling, topic modelling will then be used more for data visualisation. (or can use to make life of gpt easier lol)\n",
    "\n",
    "# Net Sentiment + Frequency of Words/Intents\n",
    "\n",
    "Alternative to NPS since we don't have raw data for NPS\n",
    "\n",
    "https://chattermill.com/blog/nps-calculator#:~:text=Calculating%20your%20net%20promoter%20score,number%20between%20%2D100%20and%20100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load database configuration from JSON file\n",
    "with open('config.json') as config_file:\n",
    "    config = json.load(config_file)['database']\n",
    "\n",
    "# Establish a connection using the loaded configuration\n",
    "cnx = mysql.connector.connect(**config)\n",
    "\n",
    "# Define your query\n",
    "query = f\"SELECT * FROM gxs_apple_app_reviews\"\n",
    "\n",
    "# Use pandas to load sql query into a DataFrame\n",
    "gxs_apple_app_gxs_reviews = pd.read_sql(query, con=cnx)\n",
    "\n",
    "# Don't forget to close the connection when done\n",
    "cnx.close()\n",
    "gxsapple=gxs_apple_app_gxs_reviews\n",
    "\n",
    "gxsapple['title_review'] = gxsapple['title'] + ' : ' + gxsapple['review']\n",
    "\n",
    "gxsapple['thumbsUp']= 0\n",
    "\n",
    "gxsapple['developerResponse'] = gxsapple['developerResponse'].apply(lambda x: x.get('body') if isinstance(x, dict) else x)\n",
    "\n",
    "gxsapple_dropped = gxsapple.drop(columns=['title', 'review', 'userName', 'isEdited'])\n",
    "\n",
    "gxsapple_dropped_renamed = gxsapple_dropped.rename(columns={\n",
    "    'title_review': 'content',\n",
    "    'thumbsUp': 'thumbsUpCount',\n",
    "    'developerResponse': 'replyContent',\n",
    "    'rating': 'score',\n",
    "    'date':'date'\n",
    "})\n",
    "\n",
    "\n",
    "# Load database configuration from JSON file\n",
    "with open('config.json') as config_file:\n",
    "    config = json.load(config_file)['database']\n",
    "\n",
    "# Establish a connection using the loaded configuration\n",
    "cnx = mysql.connector.connect(**config)\n",
    "\n",
    "# Define your query\n",
    "query = f\"SELECT * FROM gxs_playstore_app_reviews\"\n",
    "\n",
    "# Use pandas to load sql query into a DataFrame\n",
    "gxs_playstore_app_gxs_reviews = pd.read_sql(query, con=cnx)\n",
    "\n",
    "# Don't forget to close the connection when done\n",
    "cnx.close()\n",
    "\n",
    "gxsplaystore_dropped=gxs_playstore_app_gxs_reviews.drop(columns=['reviewId','userName','userImage','reviewCreatedVersion','repliedAt','appVersion'])\n",
    "neworder=['replyContent', 'score', 'content', 'thumbsUpCount']\n",
    "gxsplaystore_dropped=gxsplaystore_dropped[neworder]\n",
    "\n",
    "\n",
    "\n",
    "gxsplaystore_dropped=gxsplaystore_dropped.rename(columns={\n",
    "    'content': 'content',\n",
    "    'thumbsUpCount': 'thumbsUpCount',\n",
    "    'replyContent': 'replyContent',\n",
    "    'score': 'score',\n",
    "    'at':'date'\n",
    "})\n",
    "\n",
    "\n",
    "gxs_reviews = pd.concat([gxsplaystore_dropped, gxsapple_dropped_renamed], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  Input:\n",
    "  - reviews: dataset containing at least the following 2 columns ['content', 'thumbsUpCount']\n",
    "  Output:\n",
    "  - net sentiment score: numeric, between -100 and +100\n",
    "  - neg_keywords: dictionary containing negative keywords, sorted descending\n",
    "  - pos_keywords: dictionary containing positive keywords, sorted descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>replyContent</th>\n",
       "      <th>score</th>\n",
       "      <th>content</th>\n",
       "      <th>thumbsUpCount</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi there! Thank you for your 5-star rating rev...</td>\n",
       "      <td>5</td>\n",
       "      <td>I like how the app is really light and fast co...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi Jerry, this feature isn't ready at the mome...</td>\n",
       "      <td>1</td>\n",
       "      <td>Cannot add card to google pay. Also cannot add...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thanks for your feedback on mobile wallets and...</td>\n",
       "      <td>4</td>\n",
       "      <td>Generally good. However please add either NFC ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi Eric. We're sorry to hear this. We would li...</td>\n",
       "      <td>1</td>\n",
       "      <td>With continuous failed to log in,contacted cso...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi Hafidz Melek! Thank you for your 5-star rat...</td>\n",
       "      <td>5</td>\n",
       "      <td>I don't get the negative reviews here. I insta...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        replyContent  score  \\\n",
       "0  Hi there! Thank you for your 5-star rating rev...      5   \n",
       "1  Hi Jerry, this feature isn't ready at the mome...      1   \n",
       "2  Thanks for your feedback on mobile wallets and...      4   \n",
       "3  Hi Eric. We're sorry to hear this. We would li...      1   \n",
       "4  Hi Hafidz Melek! Thank you for your 5-star rat...      5   \n",
       "\n",
       "                                             content  thumbsUpCount date  \n",
       "0  I like how the app is really light and fast co...              0  NaT  \n",
       "1  Cannot add card to google pay. Also cannot add...              0  NaT  \n",
       "2  Generally good. However please add either NFC ...              0  NaT  \n",
       "3  With continuous failed to log in,contacted cso...              0  NaT  \n",
       "4  I don't get the negative reviews here. I insta...              0  NaT  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def summarise_sentiment(reviews):\n",
    "\n",
    "  '''\n",
    "  Input:\n",
    "  - reviews: dataset containing at least the following 2 columns ['content', 'thumbsUpCount']\n",
    "  Output:\n",
    "  - net sentiment score: numeric, between -100 and +100\n",
    "  - neg_keywords: dictionary containing negative keywords, sorted descending\n",
    "  - pos_keywords: dictionary containing positive keywords, sorted descending\n",
    "  '''\n",
    "\n",
    "  model = 'ajiayi/finetuning-sentiment-model-bank_reviews-otherbank' # maybe need to streamline this && change to trained model\n",
    "  sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model)\n",
    "  ns_score = 0\n",
    "\n",
    "  kw_model = KeyBERT() # check if this is the best model to use\n",
    "  neg_keywords = {}\n",
    "  pos_keywords = {}\n",
    "\n",
    "  for row in range(len(reviews)):\n",
    "    pred_sentiment = sentiment_pipeline(reviews.content[row])[0]['label']\n",
    "    keywords = kw_model.extract_keywords(reviews.content[row])\n",
    "    if pred_sentiment == 'POSITIVE':\n",
    "      pred_sentiment = 1\n",
    "      for word in keywords:\n",
    "        if word in pos_keywords:\n",
    "          pos_keywords[word] += 1\n",
    "        else:\n",
    "          pos_keywords[word] = 1\n",
    "    else:\n",
    "      pred_sentiment = -1\n",
    "      for word in keywords:\n",
    "        if word in neg_keywords:\n",
    "          neg_keywords[word] += 1\n",
    "        else:\n",
    "          neg_keywords[word] = 1\n",
    "    ns_score += pred_sentiment * (reviews.thumbsUpCount[row]+1) # need to vectorise for faster runtime\n",
    "\n",
    "  total = sum(reviews.thumbsUpCount) + len(reviews)\n",
    "  ns_score = ns_score/total*100\n",
    "\n",
    "  pos_keywords = dict(sorted(pos_keywords.items(), key=lambda x: x[1], reverse = True))\n",
    "  neg_keywords = dict(sorted(neg_keywords.items(), key=lambda x: x[1], reverse = True))\n",
    "\n",
    "  return ns_score, pos_keywords, neg_keywords\n",
    "\n",
    "summarise_sentiment(gxs_reviews)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
